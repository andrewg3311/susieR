---
title: "Fine-mapping with summary statistics"
author: "Yuxin Zou and Gao Wang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fine-mapping with summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,comment = "#",fig.width = 5,
                      fig.height = 3,fig.align = "center",
                      fig.cap = "&nbsp;",dpi = 120)
```

This vignette demonstrates how to use `susieR` with "summary statistics" in the context of genetic fine-mapping. We use the same simulated data as in [fine mapping vignette](finemapping.html). The simulated data is expression level of a gene ($y$) in $N \approx 600$ individuals. We want to identify with the genotype matrix $X_{N\times P}$ ($P=1000$) the genetic variables that causes changes in expression level. The data-set is shipped with `susieR`. It is simulated to have exactly 3 non-zero effects.

```{r}
library(susieR)
set.seed(1)
```

## The data-set

```{r}
data(N3finemapping)
attach(N3finemapping)
```

Notice that we've simulated 2 sets of $Y$ as 2 simulation replicates. Here we'll focus on the first data-set.

```{r}
dim(data$Y)
```

Here are the 3 "true" signals in the first data-set:

```{r}
b <- data$true_coef[,1]
plot(b, pch=16, ylab='effect size')
```

```{r}
which(b != 0)
```

So the underlying causal variables are 403, 653 and 773.

## Summary statistics from simple regression

Summary statistics of genetic association studies typically contain odds ratio (or $\hat{\beta}$ coefficient), p-value and minor allele frequencies. These information can be used to perform fine-mapping with sufficient summary statistics, which additionally require sample size, variance of phenotype, and most importantly a matrix of correlation between variables.

When the aforementioned sufficient summary statistica are provided, SuSiE can produce the exact same outcome as if individual level data were used. The correlation matrix in genetics is typically referred to as LD matrix (LD for linkage disequilibrium). One can use external reference panels to estimate it when this matrix cannot be obtained from samples directly.

Caution that the LD matrix has to be correlation matrix $r$, not $r^2$ or $abs(r)$.

Our example data-set additionally provides summary statistics for fitting per-variable univariate simple regression. The results are $\hat{\beta}$ and $SE(\hat{\beta})$ from which z-scores can be derived.
Again we focus only on results from the first data-set:

```{r}
z_scores <- sumstats[1,,] / sumstats[2,,]
z_scores <- z_scores[,1]
susie_plot(z_scores, y = "z", b=b)
```

For this example the correlation matrix can be computed directly from data provide,

```{r}
R <- cor(data$X)
```

## Fine-mapping with `susieR` using summary statistics

For starters, we assume there are at most 10 causal variables, i.e. set `L = 10`, although SuSiE is generally robust to the choice of `L`.

We also fix SuSiE prior variance to 0.1. In practice we can roughly estimate this from data. For example for QTL studies the number can be in between 1% to 20%.

### Using $\hat{\beta}$ and $\text{SE}(\hat{\beta})$

With the knowledge of variance of $y$, `susieR` can result in coefficent estimates in the original scale of data,

```{r}
fitted_bhat <- susie_bhat(bhat = sumstats[1,,1], 
                          shat = sumstats[2,,1], 
                          R = R, n = nrow(data$X), 
                          var_y = var(data$Y[,1]),
                          L = 10, 
                          scaled_prior_variance = 0.1, 
                          estimate_residual_variance = TRUE, 
                          estimate_prior_variance = FALSE, 
                          standardize = TRUE)
```

Without the knowledge of variance of $y$, `susieR` gives the coefficients in standardized $X$, $y$ scale,

```{r}
fitted_bhat_standardize <- susie_bhat(bhat = sumstats[1,,1], 
                                      shat = sumstats[2,,1], 
                                      R = R, n = nrow(data$X), 
                                      L = 10, 
                                      scaled_prior_variance = 0.1, 
                                      estimate_residual_variance = TRUE,
                                      estimate_prior_variance = FALSE)
```

Using `summary` function, we can examine the posterior inclusion probability (PIP) for each variable, and the 95% credible sets. 

Here, we are the 95% credible sets.

```{r}
summary(fitted_bhat)$cs
```

The 3 causal signals have been captured by the 3 CS reported here. The
3rd CS contains many variables, including the true causal variable
403.

We can also plot the posterior inclusion probability (PIP),

```{r}
susie_plot(fitted_bhat, y="PIP", b=b)
```

The true causal variables are colored red. The 95% CS identified are circled in different colors.

The result should be identical to using the individual level data,

```{r, fig.width=6,fig.height=6}
fitted = susie(data$X, data$Y[,1],
                L = 10,
                estimate_residual_variance = TRUE, 
                estimate_prior_variance = FALSE,
                scaled_prior_variance = 0.1)
plot(fitted$pip, fitted_bhat$pip, ylim=c(0,1))
```

```{r, fig.width=6,fig.height=6}
plot(coef(fitted), coef(fitted_bhat))
```

## Session information

Here are some details about the computing environment, including the
versions of R, and the R packages, used to generate these results.

```{r}
sessionInfo()
```
